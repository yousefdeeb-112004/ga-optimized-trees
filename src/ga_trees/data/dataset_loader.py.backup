"""
Enhanced Dataset Loader with OpenML, UCI, and Custom Format Support - COMPLETE FIXED VERSION

FILE PATH: /home/yousef/ga-optimized-trees/src/ga_trees/data/dataset_loader.py
ACTION: REPLACE THE ENTIRE FILE

Features:
- OpenML integration for 1000+ datasets
- UCI repository datasets
- CSV/Excel file loading with validation
- Automatic train/test splitting with stratification
- Data validation and type checking
- Support for imbalanced data
- Comprehensive error handling

FIXES APPLIED:
1. Reduced minimum sample requirement from 10 to dynamic based on test_size
2. Better error handling for file not found vs unknown dataset
3. Improved small dataset handling with warnings
4. Better validation messages
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Tuple, Optional, Dict, List, Union
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
import warnings
warnings.filterwarnings('ignore')


class DataValidator:
    """Validate and clean dataset."""
    
    @staticmethod
    def validate_dataset(X: np.ndarray, y: np.ndarray, 
                        task_type: str = 'classification') -> Tuple[bool, List[str]]:
        """
        Validate dataset quality and consistency.
        
        Returns:
            Tuple of (is_valid, list_of_warnings)
        """
        warnings = []
        
        # Check dimensions
        if X.shape[0] != len(y):
            return False, ["X and y have different number of samples"]
        
        if X.shape[0] == 0:
            return False, ["Dataset is empty"]
        
        # Check for NaN/Inf
        if np.any(np.isnan(X)):
            warnings.append(f"Found {np.sum(np.isnan(X))} NaN values in X")
        if np.any(np.isinf(X)):
            warnings.append(f"Found {np.sum(np.isinf(X))} Inf values in X")
        
        # Check target distribution
        if task_type == 'classification':
            unique_classes, counts = np.unique(y, return_counts=True)
            if len(unique_classes) < 2:
                return False, ["Need at least 2 classes for classification"]
            
            # Check for severe imbalance
            imbalance_ratio = counts.max() / counts.min()
            if imbalance_ratio > 10:
                warnings.append(f"Severe class imbalance detected (ratio: {imbalance_ratio:.1f}:1)")
            
            # Check for very small classes
            min_samples = counts.min()
            if min_samples < 5:
                warnings.append(f"Some classes have very few samples (min: {min_samples})")
        
        # Check feature variance
        feature_stds = np.std(X, axis=0)
        zero_var_features = np.sum(feature_stds < 1e-10)
        if zero_var_features > 0:
            warnings.append(f"Found {zero_var_features} features with zero variance")
        
        return True, warnings
    
    @staticmethod
    def clean_dataset(X: np.ndarray, y: np.ndarray, 
                     strategy: str = 'median') -> Tuple[np.ndarray, np.ndarray]:
        """
        Clean dataset by handling missing/invalid values.
        
        Args:
            X: Features
            y: Labels
            strategy: 'remove', 'mean', 'median', or 'zero'
        """
        # Handle NaN and Inf in X
        if np.any(np.isnan(X)) or np.any(np.isinf(X)):
            if strategy == 'remove':
                # Remove rows with NaN/Inf
                valid_rows = ~(np.any(np.isnan(X), axis=1) | np.any(np.isinf(X), axis=1))
                X = X[valid_rows]
                y = y[valid_rows]
            elif strategy == 'mean':
                # Replace with column mean
                col_means = np.nanmean(X, axis=0)
                for i in range(X.shape[1]):
                    mask = np.isnan(X[:, i]) | np.isinf(X[:, i])
                    X[mask, i] = col_means[i]
            elif strategy == 'median':
                # Replace with column median (more robust)
                col_medians = np.nanmedian(X, axis=0)
                for i in range(X.shape[1]):
                    mask = np.isnan(X[:, i]) | np.isinf(X[:, i])
                    X[mask, i] = col_medians[i]
            elif strategy == 'zero':
                X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        
        return X, y


class DatasetLoader:
    """
    Comprehensive dataset loader supporting multiple sources.
    
    Supports:
    - Scikit-learn built-in datasets
    - OpenML datasets (1000+ datasets)
    - UCI repository datasets
    - Custom CSV/Excel files
    - Automatic preprocessing and validation
    """
    
    # Built-in scikit-learn datasets
    SKLEARN_DATASETS = {
        'iris', 'wine', 'breast_cancer', 'digits', 'diabetes'
    }
    
    # OpenML dataset IDs (curated selection)
    OPENML_DATASETS = {
        # Classification
        'credit_g': 31,              # German Credit
        'heart': 4,                  # Heart Disease
        'diabetes_pima': 37,         # Diabetes
        'ionosphere': 59,            # Ionosphere
        'sonar': 40,                 # Sonar
        'hepatitis': 55,             # Hepatitis
        'titanic': 40945,            # Titanic
        'adult': 1590,               # Adult Income
        'mnist': 554,                # MNIST (small version)
        'credit_fraud': 1597,        # Credit Card Fraud
        
        # Additional datasets
        'vehicle': 54,               # Vehicle Silhouettes
        'balance_scale': 11,         # Balance Scale
        'blood_transfusion': 1464,   # Blood Transfusion
        'banknote': 1462,            # Banknote Authentication
        'mammographic': 310,         # Mammographic Mass
    }
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        Initialize dataset loader.
        
        Args:
            cache_dir: Directory to cache downloaded datasets
        """
        self.cache_dir = Path(cache_dir) if cache_dir else Path.home() / '.ga_trees' / 'datasets'
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.validator = DataValidator()
    
    def load_dataset(self, name: str, 
                    test_size: float = 0.2,
                    random_state: int = 42,
                    stratify: bool = True,
                    standardize: bool = False,
                    balance: Optional[str] = None) -> Dict:
        """
        Load dataset from any supported source.
        
        Args:
            name: Dataset name or path to file
            test_size: Proportion for test set (0-1)
            random_state: Random seed
            stratify: Use stratified split for classification
            standardize: Apply standardization
            balance: Balancing strategy ('oversample', 'undersample', or None)
            
        Returns:
            Dictionary with keys: X_train, X_test, y_train, y_test, 
            feature_names, target_names, metadata
        """
        print(f"Loading dataset: {name}")
        
        # Determine source and load
        if name in self.SKLEARN_DATASETS:
            X, y, feature_names, target_names = self._load_sklearn(name)
        elif name in self.OPENML_DATASETS:
            X, y, feature_names, target_names = self._load_openml(name)
        elif Path(name).exists():
            X, y, feature_names, target_names = self._load_file(name)
        else:
            # FIX: Better distinction between file paths and dataset names
            # Check if it looks like a file path
            if ('.' in name and 
                ('/' in name or '\\' in name or 
                 name.endswith('.csv') or name.endswith('.xlsx') or name.endswith('.xls'))):
                raise FileNotFoundError(f"File not found: {name}")
            else:
                # Try OpenML by name as last resort
                try:
                    X, y, feature_names, target_names = self._load_openml_by_name(name)
                except:
                    raise ValueError(f"Unknown dataset: {name}")
        
        # Validate
        valid, warnings = self.validator.validate_dataset(X, y)
        if not valid:
            raise ValueError(f"Invalid dataset: {warnings}")
        if warnings:
            print(f"\n⚠️  Data Quality Warnings:")
            for i, warning in enumerate(warnings, 1):
                print(f"  {i}. {warning}")
        
        # Clean if needed (use median by default - more robust)
        X, y = self.validator.clean_dataset(X, y, strategy='median')
        
        # FIX: Dynamic minimum dataset size based on test_size
        # More flexible than fixed minimum of 10
        min_samples_required = max(5, int(1 / test_size) + 2)
        
        if len(X) < min_samples_required:
            # Try to continue with warning for very small datasets
            if len(X) >= 5:
                print(f"\n⚠️  Warning: Small dataset ({len(X)} samples). " 
                      f"Results may not be reliable.")
            else:
                raise ValueError(
                    f"Dataset too small: {len(X)} samples. "
                    f"Need at least {min_samples_required} for test_size={test_size}. "
                    f"Consider using a smaller test_size or adding more samples."
                )
        
        # Balance if requested
        if balance:
            X, y = self._balance_dataset(X, y, strategy=balance, random_state=random_state)
        
        # Train/test split with error handling
        stratify_labels = y if stratify else None
        try:
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=stratify_labels
            )
        except ValueError as e:
            if 'stratify' in str(e).lower():
                print("⚠️  Stratification failed (likely too few samples per class). Using random split.")
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=test_size, random_state=random_state, stratify=None
                )
            else:
                raise
        
        # Standardize if requested
        scaler = None
        if standardize:
            scaler = StandardScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)
        
        # Metadata
        metadata = {
            'n_samples': len(X),
            'n_features': X.shape[1],
            'n_classes': len(np.unique(y)),
            'train_size': len(X_train),
            'test_size': len(X_test),
            'feature_names': feature_names,
            'target_names': target_names,
            'balanced': balance is not None,
            'standardized': standardize,
        }
        
        print(f"✓ Loaded: {metadata['n_samples']} samples, "
              f"{metadata['n_features']} features, {metadata['n_classes']} classes")
        
        return {
            'X_train': X_train,
            'X_test': X_test,
            'y_train': y_train,
            'y_test': y_test,
            'feature_names': feature_names,
            'target_names': target_names,
            'metadata': metadata,
            'scaler': scaler
        }
    
    def _load_sklearn(self, name: str) -> Tuple:
        """Load sklearn built-in dataset."""
        from sklearn.datasets import (
            load_iris, load_wine, load_breast_cancer, 
            load_digits, load_diabetes
        )
        
        loaders = {
            'iris': load_iris,
            'wine': load_wine,
            'breast_cancer': load_breast_cancer,
            'digits': load_digits,
            'diabetes': load_diabetes
        }
        
        data = loaders[name]()
        return (
            data.data,
            data.target,
            list(data.feature_names) if hasattr(data, 'feature_names') else None,
            list(data.target_names) if hasattr(data, 'target_names') else None
        )
    
    def _load_openml(self, name: str) -> Tuple:
        """Load dataset from OpenML."""
        dataset_id = self.OPENML_DATASETS[name]
        return self._load_openml_by_id(dataset_id)
    
    def _load_openml_by_id(self, dataset_id: int) -> Tuple:
        """Load OpenML dataset by ID."""
        try:
            data = fetch_openml(data_id=dataset_id, as_frame=True, parser='auto')
            
            df = data.data.copy()
            y = data.target.values
            
            # Encode categorical features
            for col in df.columns:
                if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                    df[col] = df[col].astype(str)
                    df[col] = df[col].replace('nan', 'missing_value')
                    le = LabelEncoder()
                    df[col] = le.fit_transform(df[col])
                elif df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    if df[col].isna().any():
                        df[col] = df[col].fillna(df[col].median())
            
            X = df.values.astype(float)
            
            # Encode labels
            if y.dtype == object or (len(y) > 0 and isinstance(y[0], str)):
                le = LabelEncoder()
                y = le.fit_transform(y.astype(str))
                target_names = list(le.classes_)
            else:
                y = y.astype(int)
                target_names = list(np.unique(y))
            
            feature_names = list(data.feature_names)
            
            return X, y, feature_names, target_names
            
        except Exception as e:
            raise RuntimeError(f"Failed to load OpenML dataset {dataset_id}: {e}")
    
    def _load_openml_by_name(self, name: str) -> Tuple:
        """Load OpenML dataset by name."""
        try:
            data = fetch_openml(name=name, version=1, as_frame=True, parser='auto')
            
            df = data.data.copy()
            y = data.target.values
            
            # Encode categorical features
            for col in df.columns:
                if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                    df[col] = df[col].astype(str)
                    df[col] = df[col].replace('nan', 'missing_value')
                    le = LabelEncoder()
                    df[col] = le.fit_transform(df[col])
                elif df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    if df[col].isna().any():
                        df[col] = df[col].fillna(df[col].median())
            
            X = df.values.astype(float)
            
            if y.dtype == object or (len(y) > 0 and isinstance(y[0], str)):
                le = LabelEncoder()
                y = le.fit_transform(y.astype(str))
                target_names = list(le.classes_)
            else:
                y = y.astype(int)
                target_names = list(np.unique(y))
            
            feature_names = list(data.feature_names)
            
            return X, y, feature_names, target_names
            
        except Exception as e:
            raise RuntimeError(f"Failed to load OpenML dataset '{name}': {e}")
    
    def _load_file(self, filepath: str) -> Tuple:
        """Load dataset from CSV or Excel file."""
        path = Path(filepath)
        
        if not path.exists():
            raise FileNotFoundError(f"File not found: {filepath}")
        
        # Load based on extension
        try:
            if path.suffix == '.csv':
                df = pd.read_csv(filepath)
            elif path.suffix in ['.xlsx', '.xls']:
                df = pd.read_excel(filepath)
            else:
                raise ValueError(f"Unsupported file format: {path.suffix}. Supported: .csv, .xlsx, .xls")
        except pd.errors.EmptyDataError:
            raise ValueError(f"File is empty: {filepath}")
        except pd.errors.ParserError as e:
            raise ValueError(f"Malformed CSV file: {filepath}\nError: {e}")
        except Exception as e:
            raise ValueError(f"Failed to load file {filepath}: {e}")
        
        # Validate loaded dataframe
        if df.empty:
            raise ValueError(f"File contains no data: {filepath}")
        if df.shape[1] < 2:
            raise ValueError(f"File must have at least 2 columns (features + target): {filepath}")
        
        print(f"Loaded file with shape: {df.shape}")
        
        # Assume last column is target
        X = df.iloc[:, :-1].values
        y = df.iloc[:, -1].values
        
        # Encode target if needed
        if y.dtype == object or isinstance(y[0], str):
            le = LabelEncoder()
            y = le.fit_transform(y)
            target_names = list(le.classes_)
        else:
            target_names = list(np.unique(y))
        
        feature_names = list(df.columns[:-1])
        
        # Encode categorical features
        for i in range(X.shape[1]):
            if isinstance(X[0, i], str):
                le = LabelEncoder()
                X[:, i] = le.fit_transform(X[:, i])
        
        X = X.astype(float)
        
        return X, y, feature_names, target_names
    
    def _balance_dataset(self, X: np.ndarray, y: np.ndarray, 
                        strategy: str, random_state: int) -> Tuple:
        """Balance dataset for imbalanced classes."""
        unique_classes, counts = np.unique(y, return_counts=True)
        
        if strategy == 'oversample':
            max_samples = counts.max()
            X_balanced, y_balanced = [], []
            
            for cls in unique_classes:
                X_cls = X[y == cls]
                y_cls = y[y == cls]
                
                if len(X_cls) < max_samples:
                    X_resampled, y_resampled = resample(
                        X_cls, y_cls, 
                        n_samples=max_samples, 
                        random_state=random_state
                    )
                    X_balanced.append(X_resampled)
                    y_balanced.append(y_resampled)
                else:
                    X_balanced.append(X_cls)
                    y_balanced.append(y_cls)
            
            X = np.vstack(X_balanced)
            y = np.hstack(y_balanced)
            
        elif strategy == 'undersample':
            min_samples = counts.min()
            X_balanced, y_balanced = [], []
            
            for cls in unique_classes:
                X_cls = X[y == cls]
                y_cls = y[y == cls]
                
                if len(X_cls) > min_samples:
                    X_resampled, y_resampled = resample(
                        X_cls, y_cls,
                        n_samples=min_samples,
                        random_state=random_state,
                        replace=False
                    )
                    X_balanced.append(X_resampled)
                    y_balanced.append(y_resampled)
                else:
                    X_balanced.append(X_cls)
                    y_balanced.append(y_cls)
            
            X = np.vstack(X_balanced)
            y = np.hstack(y_balanced)
        
        print(f"✓ Balanced dataset using {strategy}")
        return X, y
    
    @staticmethod
    def list_available_datasets() -> Dict[str, List[str]]:
        """List all available datasets."""
        return {
            'sklearn': list(DatasetLoader.SKLEARN_DATASETS),
            'openml': list(DatasetLoader.OPENML_DATASETS.keys()),
            'custom': ['CSV files', 'Excel files (.xlsx, .xls)']
        }
    
    @staticmethod
    def get_dataset_info(name: str) -> Dict:
        """Get information about a dataset."""
        loader = DatasetLoader()
        
        if name in loader.SKLEARN_DATASETS:
            source = 'sklearn'
        elif name in loader.OPENML_DATASETS:
            source = f'openml (ID: {loader.OPENML_DATASETS[name]})'
        else:
            source = 'unknown'
        
        return {
            'name': name,
            'source': source,
            'available': name in loader.SKLEARN_DATASETS or name in loader.OPENML_DATASETS
        }


# Convenience function
def load_benchmark_dataset(name: str, **kwargs) -> Dict:
    """
    Quick load function for benchmark datasets.
    
    Example:
        data = load_benchmark_dataset('titanic', test_size=0.3, standardize=True)
        X_train = data['X_train']
        y_train = data['y_train']
    """
    loader = DatasetLoader()
    return loader.load_dataset(name, **kwargs)


if __name__ == '__main__':
    # Demo usage
    print("="*70)
    print("GA-Trees Dataset Loader - FIXED VERSION")
    print("="*70)
    
    # List available datasets
    available = DatasetLoader.list_available_datasets()
    print("\nAvailable Datasets:")
    print(f"  Sklearn: {len(available['sklearn'])} datasets")
    print(f"  OpenML: {len(available['openml'])} datasets")
    
    # Load a dataset
    print("\nLoading Iris dataset...")
    data = load_benchmark_dataset('iris', test_size=0.2, standardize=True)
    
    print("\nDataset Info:")
    for key, value in data['metadata'].items():
        print(f"  {key}: {value}")
    
    print("\n✓ Dataset loader ready!")